# 大白话爬虫 
## 1. **初始化函数 `__init__`**
```python
def __init__(self):
```
**作用**：就像给爬虫做"准备工作"
- 设置要访问的网站地址
- 准备请求头（告诉网站"我是浏览器，不是机器人"）
- 创建会话（保持登录状态）
- 准备一个空列表来存放爬取的数据

## 2. **发送请求函数 `graphql_query`**
```python
def graphql_query(self, query, variables=None, max_retries=3):
```
**作用**：向LeetCode网站发送"问问题"的请求
- 把我们的问题打包成网站能理解的格式
- 如果失败会自动重试3次
- 返回网站的回答（数据）

## 3. **获取题目列表函数 `get_problems_list`**
```python
def get_problems_list(self, limit=10):
```
**作用**：获取前N道题目的基本信息
- 问网站："请给我前10道题目的列表"
- 返回每道题的：编号、标题、难度、标签等基本信息

## 4. **解析内容函数 `parse_translated_content`**
```python
def parse_translated_content(self, translated_content):
```
**作用**：把复杂的题目内容拆分成容易理解的部分
- **描述**：题目的文字说明
- **示例**：输入输出例子（就像数学题的例题）
- **约束条件**：题目限制（比如数字不能太大）
- **进阶**：更难的挑战要求

## 5. **获取题目详情函数 `get_problem_detail`**
```python
def get_problem_detail(self, title_slug):
```
**作用**：获取单道题目的完整信息
- 根据题目ID获取详细信息
- 包括：题目描述、示例、提示、代码模板等
- 如果缺少某些信息会自动补充

## 6. **简单翻译函数 `translate_content_basic`**
```python
def translate_content_basic(self, content):
```
**作用**：把英文关键词翻译成中文
- 比如把 "Example" → "示例"
- 把 "Input" → "输入"
- 让英文题目更容易理解

## 7. **格式转换函数 `convert_to_json_examples`**
```python
def convert_to_json_examples(self, examples):
```
**作用**：把示例数据转换成标准格式
- 把普通的文本示例变成JSON格式
- 方便程序处理和存储

## 8. **解析统计信息函数 `parse_stats`**
```python
def parse_stats(self, stats_str):
```
**作用**：解析题目的统计数据
- 比如通过率、提交次数等
- 把字符串转换成程序能理解的格式

## 9. **主爬取函数 `crawl_problems`**
```python
def crawl_problems(self, problem_count=10, output_file=None):
```
**作用**：整个爬虫的"总指挥"
1. 获取题目列表
2. 对每道题目获取详细信息
3. 整理数据格式
4. 控制爬取速度（避免太快被网站封禁）

## 10. **保存文件函数 `save_to_json`**
```python
def save_to_json(self, filename=None):
```
**作用**：把爬取的数据保存到文件
- 自动生成带时间戳的文件名
- 把数据整理成漂亮的JSON格式
- 保存到电脑上

## 11. **显示统计函数 `show_detailed_statistics`**
```python
def show_detailed_statistics(self):
```
**作用**：显示爬取结果的统计信息
- 爬取了多少题目
- 哪些信息获取完整，哪些缺失
- 题目难度分布
- 示例数量统计

## 12. **主函数 `main`**
```python
def main():
```
**作用**：程序的"启动按钮"
- 创建爬虫对象
- 开始爬取
- 处理异常情况
- 显示最终结果

## 简单比喻

把这个爬虫想象成一个**智能图书管理员**：

- `__init__` = 准备借书证和工作台
- `get_problems_list` = 查看图书目录
- `get_problem_detail` = 从书架上拿具体的书
- `parse_translated_content` = 把书的章节整理清楚
- `crawl_problems` = 整个借书流程的管理
- `save_to_json` = 把书的内容抄录下来
- `main` = 开始借书任务

## 查询方式的比较

|  |  |  |
|--|--|--|
|技术	| 适合场景	| 不适合场景 | 
|GraphQL	| 前端应用、移动端、需要灵活查询	| 简单CRUD、性能要求极高的场景 |
|REST API	 | 简单业务、第三方开放API	 | 复杂数据关系、频繁变更的需求 |
|HTML爬取 |	没有API的网站、数据挖掘	| 生产环境、需要稳定性的项目 |
|gRPC	| 微服务通信、高性能后端 | 	浏览器前端、快速原型开发 | 
|WebSocket |	实时聊天、股票行情、游戏	| 普通的数据查询 |
